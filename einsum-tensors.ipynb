{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3426fbeb-cfd2-4a75-976a-1e8c8dee2335",
   "metadata": {},
   "source": [
    "In this article, we provide a cheatsheet for using [einsum](https://pytorch.org/docs/stable/generated/torch.einsum.html) to perform common operations in deep learning. We start from simple operations like rowsum that serve as building blocks. And gradually build up to defining multi-head attention with einsum. Where applicable I compare with more common ways of doing the same operations in PyTorch and the problem with these ways.\n",
    "\n",
    "This post came about as I was implementing self-attention and comparing against llama3's open sourced code. llama3 code has model and data parallel to enable training large models on multiple GPUs. Like everything in PyTorch, there are mutiple ways to implement self-attention. See also, [this efficient implementation](https://github.com/facebookresearch/xformers/blob/95f085abc3ba8cfaa2527250b8e274a95b10f7fe/xformers/components/attention/core.py#L208). Here I focus on readability and exposition.\n",
    "\n",
    "I like einsum because the syntax is pithy and self-documenting. I can easily visualize the shapes of the input and output tensors leading to easier coding and debugging. See these [visualizations](https://www.tensors.net/tutorial-1) and the [original proposal](https://nlp.seas.harvard.edu/NamedTensor) for more discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1cb270-0dd6-4ef7-a8e6-74898c6cf36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "a = torch.arange(6).reshape(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b53c530-9e8b-4841-9463-24e809b026b6",
   "metadata": {},
   "source": [
    "## Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f59c24c-bb27-4790-9119-75aae891f231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3],\n",
       "        [1, 4],\n",
       "        [2, 5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij->ji', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfaf40a-3ccc-4704-86f7-5616ba87bd9a",
   "metadata": {},
   "source": [
    "## Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca925e0-2ead-4552-8d71-b44126c7ee2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij->', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3dbbb-5e6e-4afc-a44b-9717d3d80388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6510c29-efc8-4881-9023-656aa13883ee",
   "metadata": {},
   "source": [
    "## Rowsum (sum all rows, compress along rows) -> get a vector of length j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af6009e-f188-411e-a483-7d410fe639ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij->j', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c0ba615-557a-4fc4-9f32-e243c9b56cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8fd5cc-66a9-4844-993a-e57d67839a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff3bab9e-1328-48ea-9cc2-065123225e52",
   "metadata": {},
   "source": [
    "## Colsum (sum all cols, compress along cols) -> get a vector of length i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a559a5d-06a8-445b-9192-3d3448ff2af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij->i', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3136f761-8a2b-4bdd-913c-532dbccdcaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d142ef3-bf7b-40e9-9a7f-db9e87006faa",
   "metadata": {},
   "source": [
    "## Matrix vector multiplication\n",
    "\n",
    "This is a weighted sum of all columns, where column i is weighted by b[i], and similar to colsum compress all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb6f9f0b-3854-4fc3-84d3-928596390265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.arange(3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831a600e-dc3c-415c-9dbf-3f1c77143dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 14])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, j ->i', a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5a5fc-4731-4975-b35b-f89645ca6508",
   "metadata": {},
   "source": [
    "## Matrix-Matrix multiplication\n",
    "\n",
    "The index missing in 'ij, jk -> ik' on the output-string is j. This means we sum across j. To help visualize, do 'ij,j->i', k times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79ef0d15-fec3-4cba-8422-d7b5ee902dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.arange(12).reshape(3, 4)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c622c50c-5fce-40f5-9ca6-f4526c9c6e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20, 23, 26, 29],\n",
       "        [56, 68, 80, 92]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, jk -> ik', [a, c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ebf46c-1211-4b9e-a7e6-ae806010e63b",
   "metadata": {},
   "source": [
    "## Dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e04ea53-29a8-4c81-abf1-732f7119a38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b.b\n",
    "torch.einsum('j,j->',[b,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4356d-a32d-4e33-bbf1-5244cccebe89",
   "metadata": {},
   "source": [
    "## softmax($w^T M_t$)\n",
    "\n",
    "It is not obvious from reading this single line below what probs shape is. One has to rely either on comments or look at the input tensor dimensions, determine what the output shape is, and then remember it. Instead with einsum, it is clear the distribution is across j output nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f69e917e-74c7-482b-8e08-bc79f8747dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q5/_q4871x926x75g_ycdd6p9k80000gn/T/ipykernel_96466/3521217325.py:3: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3679.)\n",
      "  probs = F.softmax(w.T.matmul(M_t), dim=0)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(5)\n",
    "M_t = torch.randn(5, 7)\n",
    "probs = F.softmax(w.T.matmul(M_t), dim=0) \n",
    "\n",
    "#einsum implementation\n",
    "probs_e = F.softmax(torch.einsum(\"i,ij->j\",[w,M_t]), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73936b93-d5d0-417e-9997-7fbeaff9d401",
   "metadata": {},
   "source": [
    "## $W h$ -  matrix vector multiplication for a batch of vectors\n",
    "\n",
    "Caveat: The goal here is to show equivalence of outputs from einsum, Linear and matmul. In practice, one would rather use efficient library implementations like Linear, especially when used as part of a larger network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb1eeb51-21c5-4e96-bc51-ec297a1e19ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "ip_dim = 3\n",
    "op_dim = 5\n",
    "\n",
    "h = torch.randn(batch_size, ip_dim) #a batch of 3-dimensional vectors\n",
    "model = torch.nn.Linear(ip_dim, op_dim, bias = False)\n",
    "w = model.weight\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dfe5df4-b3ba-4f9e-8166-2fad1a0f6efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0886,  0.4270, -0.9878, -0.8810, -1.3579],\n",
       "        [ 1.0827,  0.6016, -0.7292,  0.1051, -0.1298]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_output = model(h)\n",
    "batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b41cfd0-620e-44a4-899e-61807802eb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0886,  0.4270, -0.9878, -0.8810, -1.3579],\n",
       "        [ 1.0827,  0.6016, -0.7292,  0.1051, -0.1298]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(h, w.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31f1b7cc-c0dd-4c7a-9626-2bb42ca16f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0886,  0.4270, -0.9878, -0.8810, -1.3579],\n",
       "        [ 1.0827,  0.6016, -0.7292,  0.1051, -0.1298]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"ij,jk->ik\", [h, w.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4128a79-b4ed-43b8-b149-fe0c812c0ab9",
   "metadata": {},
   "source": [
    "## Linear projection for a batch of a sequence of vectors\n",
    "\n",
    "Taking the above one step further. What if each \"example\" in our dataset is a sequence of items, and each item is a vector. e.g., In transformer-based NLP models, a sentence is a sequence of tokens (roughly) corresponding to subwords. Each token has a learned embedding. In self-attention mechanism, we first do a linear projection of the sequence of query, key and value token embeddings using $W_q$, $W_k$ and $W_v$ respectively. In this setting, ip_dim is the embedding dimension, and op_dim the head dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5618684b-070b-4d26-8ff3-6f92fc48a704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3666, -0.5485,  0.8776,  0.2649,  0.6922],\n",
       "         [ 0.1655, -0.4287,  0.3336, -0.7282, -0.4033],\n",
       "         [ 0.1926, -0.2491,  0.1000, -0.6668, -0.4860],\n",
       "         [ 0.7255,  0.1736,  0.0437,  0.5366,  0.6470],\n",
       "         [ 0.5413, -0.3512,  0.3835, -0.4290, -0.0329],\n",
       "         [-0.2362,  0.1570, -0.1873,  0.1530, -0.0279]],\n",
       "\n",
       "        [[ 0.7696, -0.3414,  0.0927, -1.1343, -0.7833],\n",
       "         [-1.1330, -0.3537,  0.5483,  0.3339,  0.3955],\n",
       "         [ 0.2123,  0.0751,  0.0408,  0.3067,  0.3380],\n",
       "         [-0.0067, -0.1750,  0.2831,  0.0667,  0.2296],\n",
       "         [ 0.9058,  0.2376, -0.1498,  0.2591,  0.3221],\n",
       "         [-0.1460,  1.0374, -1.0594,  1.1088,  0.3000]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 6\n",
    "sequence_data = torch.randn(batch_size, seq_length, ip_dim)\n",
    "model(sequence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0c6f79e-7407-4619-ad24-1a51eceda1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3666, -0.5485,  0.8776,  0.2649,  0.6922],\n",
       "         [ 0.1655, -0.4287,  0.3336, -0.7282, -0.4033],\n",
       "         [ 0.1926, -0.2491,  0.1000, -0.6668, -0.4860],\n",
       "         [ 0.7255,  0.1736,  0.0437,  0.5366,  0.6470],\n",
       "         [ 0.5413, -0.3512,  0.3835, -0.4290, -0.0329],\n",
       "         [-0.2362,  0.1570, -0.1873,  0.1530, -0.0279]],\n",
       "\n",
       "        [[ 0.7696, -0.3414,  0.0927, -1.1343, -0.7833],\n",
       "         [-1.1330, -0.3537,  0.5483,  0.3339,  0.3955],\n",
       "         [ 0.2123,  0.0751,  0.0408,  0.3067,  0.3380],\n",
       "         [-0.0067, -0.1750,  0.2831,  0.0667,  0.2296],\n",
       "         [ 0.9058,  0.2376, -0.1498,  0.2591,  0.3221],\n",
       "         [-0.1460,  1.0374, -1.0594,  1.1088,  0.3000]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.einsum('ilj,jk->ilk',[sequence_data, w.T]) #output shape [batch size, seq length, op_dim] i.e., [2, 6, 5]\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a17de6-d1de-42fc-a2b4-d70b44f17215",
   "metadata": {},
   "source": [
    "## Now consider the self-attention mechanism across multiple attention heads\n",
    "\n",
    "We want to process the same sequence with multiple attention heads, each first doing a linear projection with corresponding weights. But why do n_head multiplications, when we could do one giant multiplication and use GPUs more efficiently? So, we do a _single_ linear projection into n_head*head_dim, i.e. $4*5=20$. Then, we view results spit across n_heads (4). This allows us to compute scores using softmax independently for each head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8acc17a7-c1e0-4ebe-a9d9-7a03bbc61695",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads= 4\n",
    "head_dim = op_dim\n",
    "wq = torch.nn.Linear(ip_dim, n_heads*op_dim, bias = False)\n",
    "\n",
    "wk = torch.nn.Linear(ip_dim, n_heads*op_dim, bias = False)\n",
    "wv = torch.nn.Linear(ip_dim, n_heads*op_dim, bias = False)\n",
    "wo = torch.nn.Linear(n_heads*head_dim, ip_dim, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93118677-3c71-49d8-b2fb-b95398045be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 20])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq = wq(sequence_data)\n",
    "xq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a99adb5-0b62-4a7d-8349-40e191db27af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 4, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq = xq.view(batch_size, seq_length, n_heads, head_dim)\n",
    "xq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1b8c3-34ae-4aa6-899a-6a35b763fbf8",
   "metadata": {},
   "source": [
    "Similarly, compute keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71dcd965-5a2f-4ae9-bc97-0ecb87c08b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = wk(sequence_data)\n",
    "values = wv(sequence_data)\n",
    "\n",
    "keys = keys.view(batch_size, seq_length, n_heads, head_dim)\n",
    "values = values.view(batch_size, seq_length, n_heads, head_dim)\n",
    "\n",
    "# Rearrange tensors so we compute one set of scores per attention-head\n",
    "xq = xq.transpose(1, 2)  #(bs, n_heads, seqlen, head_dim)\n",
    "keys = keys.transpose(1, 2)  #(bs, n_heads, seqlen, head_dim)\n",
    "values = values.transpose(1, 2)  #(bs, n_heads, seqlen, head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850a8e8f-51a0-4d21-886e-360986d3e3a3",
   "metadata": {},
   "source": [
    "Compute scores for all pairs of tokens in the sequence. For matmul between xq and keys to produce a seq_len*seq_len set of scores, we need to transpose keys so it has shape #(bs, n_heads, head_dim, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d578892-ac9a-496a-a604-795cc2e1fae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6, 6])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51995d-9664-46c4-9a67-c7b9ac0e499c",
   "metadata": {},
   "source": [
    "Compute attention-weighted vector of the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c94c39b5-3bf1-4458-ac23-2bd6f03cfece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6, 5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.matmul(scores, values)  # (bs, n_heads, seqlen, head_dim)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e5727-3a94-4a1a-aa4d-8d1f9251ac1c",
   "metadata": {},
   "source": [
    "Concatenate outputs from multiple heads so we get an output tensor of shape (batch_size, seq_length, n_heads*head_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5b14144-418b-426c-88f6-68a5683a9e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 20])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output.transpose(1, 2).contiguous().view(batch_size, seq_length, -1)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeae9c1-1e35-44dc-80fe-aa8e40c4212f",
   "metadata": {},
   "source": [
    "And one final linear projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c652637-f03a-41f0-b325-702da97e628d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wo(output).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a9917c-e002-4b71-947c-b1cdbcfacebf",
   "metadata": {},
   "source": [
    "## Einsum implementation of multi-head self-attention\n",
    "\n",
    "The index that is missing in the output string is the one that is being compressed or summed over. So note in output_e computation below bhlj, bhjd -> bhld. Here we want to compress/sum over j. Perhaps it's easier to use the ellipsis notation, which brings the focus to lj, jd and now we can think again in 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7cd2dd5-c9af-4979-9999-777c915dd6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores shape: torch.Size([2, 4, 6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xqq = wq(sequence_data).view(batch_size, seq_length, n_heads, head_dim)\n",
    "kk  = wk(sequence_data).view(batch_size, seq_length, n_heads, head_dim)\n",
    "vv  = wv(sequence_data).view(batch_size, seq_length, n_heads, head_dim)\n",
    "\n",
    "xqq = torch.einsum('blhd->bhld',[xqq])\n",
    "kk  = torch.einsum('blhd->bhld',[kk])\n",
    "vv  = torch.einsum('blhd->bhld', [vv])\n",
    "\n",
    "scores_e = torch.einsum(\"bhid, bhjd -> bhij\", [xqq, kk]) / math.sqrt(head_dim)\n",
    "scores_e = F.softmax(scores_e.float(), dim =-1).type_as(xq)\n",
    "\n",
    "print(f\"Scores shape: {scores_e.shape}\")\n",
    "output_e = torch.einsum(\"bhlj,bhjd->bhld\", scores_e, vv) \n",
    "output_e = torch.einsum(\"bhld->blhd\", [output_e]).contiguous().view(batch_size, seq_length, -1)\n",
    "output == output_e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133da69-5a7c-44c6-b040-b9f853aae44e",
   "metadata": {},
   "source": [
    "It is easier to see the crux of the operation with the ellipsis notation. We ignore the batch and head dimensions, and see we are summing across the j dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "819d0d50-64eb-47bf-a48d-36512be1cde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ee = torch.einsum(\"...lj,...jd->...ld\", scores_e, vv) \n",
    "output_ee = torch.einsum(\"bhld->blhd\", [output_ee]).contiguous().view(batch_size, seq_length, -1)\n",
    "output == output_ee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abb317-22b9-4bda-b78b-936b3da3aa6a",
   "metadata": {},
   "source": [
    "In conclusion, einsum is a nifty tool to implement and visualize almost any tensor operation. An even more versatile tool is einops, that I plan to cover in a subsequent post."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
